{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Download VADER lexicon if not already installed\n",
    "nltk.download(\"vader_lexicon\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "INTERIM_DIR = \"data/interim\"\n",
    "PREPROCESSED_FILE = os.path.join(INTERIM_DIR, \"reddit_preprocessed.csv\")\n",
    "WRANGLED_FILE = os.path.join(INTERIM_DIR, \"reddit_wrangled.csv\")\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv(PREPROCESSED_FILE, parse_dates=[\"created\"])\n",
    "print(f\"Loaded preprocessed data: {df.shape[0]} rows, {df.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = df[\"created\"].dt.year\n",
    "df[\"month\"] = df[\"created\"].dt.month\n",
    "df[\"day\"] = df[\"created\"].dt.day\n",
    "df[\"weekday\"] = df[\"created\"].dt.weekday\n",
    "df[\"hour\"] = df[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure text column is string\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
    "\n",
    "df[\"word_count\"] = df[\"text\"].str.split().apply(len)\n",
    "df[\"char_count\"] = df[\"text\"].str.len()\n",
    "df[\"avg_word_len\"] = df[\"char_count\"] / df[\"word_count\"].replace(0, 1)\n",
    "df[\"has_url\"] = df[\"text\"].str.contains(r\"http\", regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engagement Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure text column is string\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
    "\n",
    "df[\"word_count\"] = df[\"text\"].str.split().apply(len)\n",
    "df[\"char_count\"] = df[\"text\"].str.len()\n",
    "df[\"avg_word_len\"] = df[\"char_count\"] / df[\"word_count\"].replace(0, 1)\n",
    "df[\"has_url\"] = df[\"text\"].str.contains(r\"http\", regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author and Subreddit Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posts per author\n",
    "author_post_count = (\n",
    "    df[df[\"source\"] == \"post\"].groupby(\"author\").size().rename(\"author_post_count\")\n",
    ")\n",
    "df = df.merge(author_post_count, on=\"author\", how=\"left\")\n",
    "\n",
    "# Comments per author\n",
    "author_comment_count = (\n",
    "    df[df[\"source\"] == \"comment\"]\n",
    "    .groupby(\"author\")\n",
    "    .size()\n",
    "    .rename(\"author_comment_count\")\n",
    ")\n",
    "df = df.merge(author_comment_count, on=\"author\", how=\"left\")\n",
    "\n",
    "# Posts per subreddit\n",
    "subreddit_post_count = (\n",
    "    df[df[\"source\"] == \"post\"]\n",
    "    .groupby(\"subreddit\")\n",
    "    .size()\n",
    "    .rename(\"subreddit_post_count\")\n",
    ")\n",
    "df = df.merge(subreddit_post_count, on=\"subreddit\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain / URL Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"main_domain\"] = df[\"url\"].str.extract(r\"https?://([^/]+)/\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "df[\"sentiment_compound\"] = df[\"text\"].apply(\n",
    "    lambda x: sia.polarity_scores(str(x))[\"compound\"]\n",
    ")\n",
    "df[\"sentiment_label\"] = df[\"sentiment_compound\"].apply(\n",
    "    lambda x: \"positive\" if x > 0 else (\"negative\" if x < 0 else \"neutral\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Keyword / Topic Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[\"topic\"] = np.nan  # can fill later using BERTopic or LDA\n",
    "df[\"stance\"] = np.nan  # optional stance labeling for misinformation analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Wrangled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(WRANGLED_FILE, index=False)\n",
    "print(f\"Wrangled dataset saved: {WRANGLED_FILE}\")\n",
    "print(f\"Final shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
